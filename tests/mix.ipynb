{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"../data/train.csv\").drop(\"id\",axis=1)\n",
    "df_test_raw = pd.read_csv(\"../data/test.csv\").drop(\"id\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (61878, 93)\n",
      "y shape : (61878,)\n"
     ]
    }
   ],
   "source": [
    "# Récupération des labels\n",
    "labels = df_raw[\"target\"].values\n",
    "y = []\n",
    "for label in labels:\n",
    "    y.append(int(label[-1:])-1)\n",
    "y = np.asarray(y)\n",
    "# Récupérations des features sous forme de ndarray\n",
    "df_raw = df_raw.drop(\"target\", axis=1)\n",
    "X = df_raw.values\n",
    "# X_test = df_test_raw.values\n",
    "print('X shape : '+str(X.shape))\n",
    "print('y shape : '+str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bin_model(X, y, c):\n",
    "    X_with_c = X[np.where(y == (c - 1))]\n",
    "    X_without_c = X[np.where(y != (c-1))]\n",
    "    np.random.shuffle(X_without_c)\n",
    "    X_without_c = X_without_c[:X_with_c.shape[0]]\n",
    "    print('X shapes : ')\n",
    "    print(X_with_c.shape)\n",
    "    print(X_without_c.shape)\n",
    "    X = np.concatenate((X_with_c, X_without_c), axis=0)\n",
    "    print(X.shape)\n",
    "    print('y shape : ')\n",
    "    y_1 = np.array([1 for i in range(X_with_c.shape[0])])\n",
    "    y_2 = np.array([0 for i in range(X_without_c.shape[0])])\n",
    "    y = np.concatenate((y_1, y_2))\n",
    "    print(y.shape)\n",
    "    X_norm = Normalizer().fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2,\n",
    "    random_state=42)\n",
    "    \n",
    "    clf_tmp_1 = ExtraTreesClassifier(n_estimators=20, random_state=0, n_jobs=-1)\n",
    "    clf_tmp_2 = CalibratedClassifierCV(clf_tmp_1, cv=5)\n",
    "\n",
    "    clf = AdaBoostClassifier(clf_tmp_2,\n",
    "    n_estimators=500,\n",
    "    learning_rate=1.0)\n",
    "    %time clf.fit(X_train, y_train)\n",
    "\n",
    "    pred_for_acc = clf.predict(X_test)\n",
    "\n",
    "    #print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "    print(classification_report(y_test, pred_for_acc, target_names=['class_'+str(c-1), 'other']))\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_proba(clf, c,X_test):\n",
    "    preds = []\n",
    "    pb = clf.predict_proba(X_test)\n",
    "    for p in pb:\n",
    "        pred = [0.0 for i in range(9)]\n",
    "        pred[c-1] = p[0]\n",
    "        preds.append(pred)\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mix_proba(pred_1, pred_2, coef_1, coef_2):\n",
    "    a1 = (pred_1 * coef_1)\n",
    "    #print(a1.shape)\n",
    "    a2 = (pred_2 * coef_2)\n",
    "    #print(a2.shape)\n",
    "    s = (pred_1 * coef_1) + (pred_2 * coef_2)\n",
    "    #print(s.shape)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mix_proba_lin(pred_1, pred_2, coef):\n",
    "    return ((coef * pred_1) + (1 - coef) * pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shapes : \n",
      "(2691, 93)\n",
      "(2691, 93)\n",
      "(5382, 93)\n",
      "y shape : \n",
      "(5382,)\n",
      "CPU times: user 1.59 s, sys: 144 ms, total: 1.74 s\n",
      "Wall time: 2.23 s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class_3       0.89      0.84      0.86       510\n",
      "      other       0.86      0.91      0.89       567\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = get_bin_model(X,y,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_norm = Normalizer().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "target_names = ['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = encode_proba(test, 4, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12376, 9)\n"
     ]
    }
   ],
   "source": [
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit terminé\n",
      "0.47\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class_1       0.78      0.58      0.66       399\n",
      "    class_2       0.77      0.84      0.80      3178\n",
      "    class_3       0.62      0.59      0.61      1561\n",
      "    class_4       0.81      0.55      0.65       538\n",
      "    class_5       0.97      0.98      0.97       565\n",
      "    class_6       0.94      0.95      0.94      2884\n",
      "    class_7       0.74      0.69      0.72       552\n",
      "    class_8       0.90      0.92      0.91      1674\n",
      "    class_9       0.87      0.89      0.88      1025\n",
      "\n",
      "avg / total       0.83      0.83      0.82     12376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_tmp = ExtraTreesClassifier(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "clf = CalibratedClassifierCV(clf_tmp, method='isotonic', cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"%.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, knn_pred = pickle.load(open('../data/knn300.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12376, 9)\n",
      "(12376, 9)\n"
     ]
    }
   ],
   "source": [
    "print(knn_pred.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_4 = [0.0 for i in range(9)]\n",
    "coef_4[4-1] = 0.9\n",
    "coef = [1-x for x in coef_4]\n",
    "coef_4 = np.array(coef_4)\n",
    "coef = np.array(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = mix_proba(p, pred, coef_4, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed == pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144368, 9)\n",
      "(144368, 9)\n"
     ]
    }
   ],
   "source": [
    "svm = pickle.load(open('../data/svm.p', 'rb'))\n",
    "print(svm.shape)\n",
    "ext = pickle.load(open('../data/extdc.p', 'rb'))\n",
    "print(ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [144368, 12376]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-553ca72fd99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmix_proba_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \"\"\"\n\u001b[1;32m   1640\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [144368, 12376]"
     ]
    }
   ],
   "source": [
    "for c in range(100):\n",
    "    cc = c / 100.0\n",
    "    m = mix_proba_lin(ext,svm, cc)\n",
    "    print(\"%.2f\" % log_loss(y_test, m, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.98169192  0.          0.          0.\n",
      "  0.          0.        ]\n",
      "[[ 1.   1.   1.   1.   0.3  1.   1.   1.   1. ]]\n",
      "[ 0.98169192]\n"
     ]
    }
   ],
   "source": [
    "print(p[2])\n",
    "print(coef)\n",
    "c = coef.reshape(9,1)\n",
    "pp = np.dot(p, c)\n",
    "print((pp[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2691, 93)\n",
      "(2691, 93)\n"
     ]
    }
   ],
   "source": [
    "print(X_4.shape)\n",
    "X_r = X[np.where(y != 3)]\n",
    "np.random.shuffle(X_r)\n",
    "X_not_4 = X_r[:X_4.shape[0]]\n",
    "print(X_not_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5382, 93)\n",
      "(5382,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_4, X_not_4), axis=0)\n",
    "y_1 = np.array([1 for i in range(X_4.shape[0])])\n",
    "y_2 = np.array([0 for i in range(X_4.shape[0])])\n",
    "y = np.concatenate((y_1, y_2))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_norm = Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "target_names = ['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 20.6 ms, total: 1.26 s\n",
      "Wall time: 1.3 s\n",
      "Accuracy : 0.81\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class_1       0.88      0.69      0.77       510\n",
      "    class_2       0.77      0.91      0.83       567\n",
      "\n",
      "avg / total       0.82      0.81      0.80      1077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nour/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 9\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear', C=1.0)\n",
    "%time clf.fit(X_train, y_train)\n",
    "#%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "#print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 89.2 ms, total: 1.28 s\n",
      "Wall time: 1.76 s\n",
      "Accuracy : 0.87\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class_1       0.89      0.83      0.86       510\n",
      "    class_2       0.86      0.91      0.88       567\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nour/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 9\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "#clf = RandomForestClassifier(n_estimators=30, n_jobs=-1)\n",
    "#clf_tmp = ExtraTreesClassifier(n_estimators=30, random_state=0, n_jobs=-1)\n",
    "#clf = CalibratedClassifierCV(clf_tmp, method='isotonic', cv=5)\n",
    "clf_tmp_1 = ExtraTreesClassifier(n_estimators=20, random_state=0, n_jobs=-1)\n",
    "clf_tmp_2 = CalibratedClassifierCV(clf_tmp_1, cv=5)\n",
    "\n",
    "clf = AdaBoostClassifier(clf_tmp_2,\n",
    "    n_estimators=500,\n",
    "    learning_rate=1.0)\n",
    "%time clf.fit(X_train, y_train)\n",
    "#%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "#print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
