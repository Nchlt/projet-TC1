{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Otto \n",
    "Malik Kazi Aoual & Nouredine Nour\n",
    "M2 AIC\n",
    "\n",
    "Ce notebook résume les différentes étapes de notre cheminement tout au long de ce projet. Il suit le plan du rapport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Donnés\n",
    "\n",
    "Todo : \n",
    "1. Histogramme de représentation des classes dans les données\n",
    "2. Mise en évidence des cas extrèmes df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"../data/train.csv\").drop(\"id\",axis=1)\n",
    "df_test_raw = pd.read_csv(\"../data/test.csv\").drop(\"id\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Résultats préliminaires\n",
    "\n",
    "Todo: \n",
    "1. Multinomial Naive Bayes avec et sans normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnés \n",
    "df_raw = pd.read_csv(\"../data/train.csv\").drop(\"id\",axis=1)\n",
    "df_test_raw = pd.read_csv(\"../data/test.csv\").drop(\"id\",axis=1)\n",
    "# Récupération des labels\n",
    "labels = df_raw[\"target\"].values\n",
    "y = []\n",
    "for label in labels:\n",
    "    y.append(int(label[-1:])-1)\n",
    "y = np.asarray(y)\n",
    "# Récupérations des features sous forme de ndarray\n",
    "df_raw = df_raw.drop(\"target\", axis=1)\n",
    "X = df_raw.values\n",
    "# X_test = df_test_raw.values\n",
    "print('X shape : '+str(X.shape))\n",
    "print('y shape : '+str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pour validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "target_names = ['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NE PAS LANCER (sauf si vous avez tout votre temps...)\n",
    "clf = SVC(probability=True)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes sans normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes avec normalisation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "X_train_norm = Normalizer().fit_transform(X_train)\n",
    "X_test_norm = Normalizer().fit_transform(X_test)\n",
    "%time clf.fit(X_train_norm, y_train)\n",
    "print(\"Fit terminé\")\n",
    "pred = clf.predict_proba(X_test_norm)\n",
    "pred_for_acc = clf.predict(X_test_norm)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ExtraTreesClassifier(n_estimators=250, n_jobs=-1)\n",
    "X_train_norm = Normalizer().fit_transform(X_train)\n",
    "X_test_norm = Normalizer().fit_transform(X_test)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "%time clf.fit(X_train_norm, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test_norm)\n",
    "pred_for_acc = clf.predict(X_test_norm)\n",
    "print(\"Log loss (Kaggle metric) : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Arbres et calibration\n",
    "\n",
    "Todo :\n",
    "1. lancer la meilleur repres et eventuellement la tester en cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tmp = ExtraTreesClassifier(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "clf = CalibratedClassifierCV(clf_tmp, method='isotonic', cv=5)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"%.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Feature selection et Nouvelle représentation\n",
    "\n",
    "Todo:\n",
    "1. Relancer les algos précédents avec %time pour voir \n",
    "2. SVM\n",
    "3. KNN eventuellement CV pour le K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séléction des features\n",
    "\n",
    "Afin de pouvoir lancer les classifieurs tels que KNN ou SVM trop lents sur la représentation initiale, nous avons voulu tenter de réduire le nombre de features pour alléger la représentation des données.\n",
    "\n",
    "Todo :\n",
    "1. récupérer les données Selction chi2\n",
    "2. récupérer les données RMVE\n",
    "3. tracer en fonction du nombre de features gardés les perf de différent classifieur avec %time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection sur MNB\n",
    "\n",
    "X_norm = Normalizer().fit_transform(X)\n",
    "k_bests=[5, 9, 20 ,30, 50, 80, 93]\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "clf = mnb\n",
    "results_mnb = []\n",
    "for k_best in k_bests:\n",
    "    selection = SelectKBest(chi2, k=k_best)\n",
    "    X_new = selection.fit_transform(X_norm, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2,\n",
    "                                                      random_state=42)\n",
    " \n",
    "    print('MNB for k_best = '+str(k_best))\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    %time pred = clf.predict_proba(X_test)\n",
    "    results_mnb.append(log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "    print(\"--- Log loss (Kaggle metric) : %.2f ---\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection sur KNN k = 10\n",
    "\n",
    "X_norm = Normalizer().fit_transform(X)\n",
    "k_bests=[5, 9, 20 ,30, 50, 80, 93]\n",
    "knn = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "clf = knn\n",
    "results_knn = []\n",
    "for k_best in k_bests:\n",
    "    selection = SelectKBest(chi2, k=k_best)\n",
    "    X_new = selection.fit_transform(X_norm, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2,\n",
    "                                                      random_state=42)\n",
    " \n",
    "    print('KNN for k_best = '+str(k_best))\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    %time pred = clf.predict_proba(X_test)\n",
    "    results_knn.append(log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "    print(\"--- Log loss (Kaggle metric) : %.2f ---\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection sur KNN k = 10\n",
    "\n",
    "X_norm = Normalizer().fit_transform(X)\n",
    "k_bests=[9]\n",
    "svm = SVC(C=100, gamma=0.001)\n",
    "clf = svm\n",
    "results_knn = []\n",
    "for k_best in k_bests:\n",
    "    selection = SelectKBest(chi2, k=k_best)\n",
    "    X_new = selection.fit_transform(X_norm, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2,\n",
    "                                                      random_state=42)\n",
    " \n",
    "    print('SVC for k_best = '+str(k_best))\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    %time pred = clf.predict_proba(X_test)\n",
    "    results_knn.append(log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "    print(\"--- Log loss (Kaggle metric) : %.2f ---\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection sur RF 100\n",
    "\n",
    "X_norm = Normalizer().fit_transform(X)\n",
    "k_bests=[5, 9, 20 ,30, 50, 80, 93]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "clf = rf\n",
    "results_rf = []\n",
    "for k_best in k_bests:\n",
    "    selection = SelectKBest(chi2, k=k_best)\n",
    "    X_new = selection.fit_transform(X_norm, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2,\n",
    "                                                      random_state=42)\n",
    " \n",
    "    print('RF for k_best = '+str(k_best))\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    %time pred = clf.predict_proba(X_test)\n",
    "    results_rf.append(log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "    print(\"--- Log loss (Kaggle metric) : %.2f ---\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(k_bests, results_mnb, color='r', label='Multinomial Naive Bayes')\n",
    "plt.plot(k_bests, results_rf, color='black', label='Random Forest')\n",
    "plt.plot(k_bests, results_knn, color='b', label='KNN')\n",
    "plt.xlabel('Nombre de features gardées dans la représentation')\n",
    "plt.ylabel('Log loss')\n",
    "plt.legend()\n",
    "plt.plot\n",
    "plt.savefig('plot.jpg', format='jpg', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('plot.jpg', format='jpg', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = np.array(results_knn)\n",
    "results_mnb = np.array(results_mnb)\n",
    "results_rf = np.array(results_rf)\n",
    "print(results_mnb.min())\n",
    "print(results_knn.min())\n",
    "print(results_rf.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouvelle représentation Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(61878,)\n",
      "Construction des centroïdes\n",
      "(9, 93)\n",
      "Construction terminée\n",
      "X_train shape :  (49502, 9)\n",
      "X_test shape :  (12376, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\").drop(\"id\",axis=1)\n",
    "df_test = pd.read_csv(\"../data/test.csv\").drop(\"id\",axis=1)\n",
    "\n",
    "labels = df[\"target\"].values\n",
    "print(type(labels))\n",
    "print(labels.shape)\n",
    "\n",
    "df_train = df.drop(\"target\", axis=1)\n",
    "\n",
    "X = df_train.values\n",
    "#X_test = df_test.values\n",
    "\n",
    "# print(X.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "y = []\n",
    "for label in labels:\n",
    "    y.append(int(label[-1:])-1)\n",
    "y = np.asarray(y)\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "\n",
    "print(\"Construction des centroïdes\")\n",
    "\n",
    "X_tf = tf.fit_transform(X)\n",
    "\n",
    "centroids = []\n",
    "for label in unique_labels:\n",
    "    same_labels = X_tf[np.where(y == label)]\n",
    "    centroids.append(same_labels.sum(axis = 0))\n",
    "centroids = np.asarray(centroids).reshape(9,93)\n",
    "print(centroids.shape)\n",
    "print(\"Construction terminée\")\n",
    "X_dc = pairwise_distances(X_tf, centroids, metric='cosine')\n",
    "#X_test_dc = pairwise_distances(X_test, centroids, metric='minkowski')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dc, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape : \", X_train.shape)\n",
    "print(\"X_test shape : \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dc, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "target_names = ['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 ms, sys: 13.6 ms, total: 41.5 ms\n",
      "Wall time: 26.4 ms\n",
      "CPU times: user 25.7 ms, sys: 13.1 ms, total: 38.9 ms\n",
      "Wall time: 17.6 ms\n",
      "Kaggle log loss metric : 1.71\n",
      "Accuracy : 0.47\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class_1       0.00      0.00      0.00       399\n",
      "    class_2       0.50      0.99      0.66      3178\n",
      "    class_3       0.00      0.00      0.00      1561\n",
      "    class_4       0.00      0.00      0.00       538\n",
      "    class_5       0.00      0.00      0.00       565\n",
      "    class_6       0.45      0.94      0.61      2884\n",
      "    class_7       0.00      0.00      0.00       552\n",
      "    class_8       0.00      0.00      0.00      1674\n",
      "    class_9       0.00      0.00      0.00      1025\n",
      "\n",
      "avg / total       0.23      0.47      0.31     12376\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nour/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "%time clf.fit(X_train, y_train)\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "%time clf.fit(X_train, y_train)\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le random forest sur la nouvelle représentation pour 9 features fait 0.81 alors qu'il avait donné 3.26 sur la représentation à 9 features (chi2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "%time clf.fit(X_train, y_train)\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn fait 1.86 au lieu de 4.46. De plus le temps de computation est grandement diminué ce qui nous permettera d'eventuellement faire de la cv sur le K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tmp = ExtraTreesClassifier(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "clf = CalibratedClassifierCV(clf_tmp, method='isotonic', cv=5)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"%.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tmp = RandomForestClassifier(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "clf = CalibratedClassifierCV(clf_tmp, method='isotonic', cv=5)\n",
    "%time clf.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"%.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = SVC(gamma=0.001, C=100, probability=True, cache_size=4000, verbose=True, \n",
    "            class_weight='balanced')\n",
    "%time clf_2.fit(X_train, y_train)\n",
    "print(\"Fit terminé\")\n",
    "%time pred_2 = clf_2.predict_proba(X_test)\n",
    "pred_for_acc_2 = clf_2.predict(X_test)\n",
    "print(\"%.2f\" % log_loss(y_test, pred_2, eps=1e-15, normalize=True))\n",
    "print(classification_report(y_test, pred_for_acc_2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "param_dist = {\"n_neighbors\": sp_randint(1, 500)}\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(knn, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5)\n",
    "random_search.fit(X_dc, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % (2, n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [5, 10, 15, 20, 30, 40, 100, 200, 300, 400]\n",
    "cv_scores = []\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_dc, y, cv=5, scoring='log_loss')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(neighbors, cv_scores)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "cv_scores = []\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_dc, y, cv=5, scoring='log_loss')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(neighbors, cv_scores)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=18, n_jobs=-1)\n",
    "%time clf.fit(X_train, y_train)\n",
    "%time pred = clf.predict_proba(X_test)\n",
    "pred_for_acc = clf.predict(X_test)\n",
    "print(\"Kaggle log loss metric : %.2f\" % log_loss(y_test, pred, eps=1e-15, normalize=True))\n",
    "print(\"Accuracy : %.2f\" % accuracy_score(y_test, pred_for_acc))\n",
    "print(classification_report(y_test, pred_for_acc, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Mélange\n",
    "Todo:\n",
    "Récupérer les différents classifieurs précédent en pickle et tenter des mélanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
